# -*- coding: utf-8 -*-
"""M6_ex04.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EHO1r8ruMW-slcLi0o8PnK5VZgeuv5Tr

## Title : Dockerizing an MLflow-based ML App with SQLite and Streamlit Interface (Optionally, a Three-layer App), and Uploading to Docker Hub

### Task
Choose a previous project that involves a machine learning component and perform the following tasks:

1. Train a machine learning model using the data from your previous project. Select an appropriate machine learning model based on your data and problem.
- we used previous project(Penguin) with machine learning model(SVC, Randomforestclassifier)

2. Integrate MLflow for tracking and managing your machine learning experiments. Log hyperparameters, metrics, and artifacts of your experiments in MLflow. Save structured and unstructured information related to your trained model in SQLite within MLflow.

3. Develop a user-friendly interface for your ML app using Streamlit. Optionally, you can create a three-layer ML app (data, business, presentation) for a user-friendly interface to interact with the machine learning model.

4. Dockerize your ML app, ensuring that the SQLite database, MLflow, and the Streamlit or custom interface are all functioning correctly within the Docker image.

5. Upload your dockerized app to Docker Hub and provide instructions for running the app from the Docker Hub repository.
"""

import pandas as pd
import numpy as np
# load data
data = pd.read_csv("https://github.com/allisonhorst/palmerpenguins/raw/5b5891f01b52ae26ad8cb9755ec93672f49328a8/data/penguins_size.csv")
data = data.dropna()

data.head()

data['species_short'].unique()

data.describe()

"""# Datalayer : Database.py

"""

#database.py
import sqlite3
import pandas as pd
import numpy as np

# load data
data = pd.read_csv("https://github.com/allisonhorst/palmerpenguins/raw/5b5891f01b52ae26ad8cb9755ec93672f49328a8/data/penguins_size.csv")
data = data.dropna()

def init_db():
  # Load the penguin dataset into a Pandas DataFrame
  df = data[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'species_short']]

  # Connect to the SQLite database
  conn = sqlite3.connect("penguin.db")

  # Save the Pandas DataFrame to the SQLite database
  df.to_sql("penguin", conn, if_exists="replace", index=False)

  # Close the connection to the SQLite database
  conn.close()

if __name__ == '__main__':
    init_db()

"""# Business layers

1. ml_model.py
2. mlflow.py
"""

!pip install mlflow --q

# business model(ml_model.py)

import pandas as pd
import sqlite3
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
import pickle

# Connect to the SQLite database
conn = sqlite3.connect("penguin.db")

# Read data from a table using Pandas
data_df = pd.read_sql("SELECT * FROM penguin", conn)

def train_model():
    X = data_df.drop('species_short', axis=1)
    y = data_df['species_short']

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    clf = SVC()
    clf.fit(X_train, y_train)

    with open("model.pkl", "wb") as f:
        pickle.dump(clf, f)

    return clf.score(X_test, y_test)

if __name__ == '__main__':
    accuracy = train_model()
    print(f"Model trained with accuracy: {accuracy}")

# ML_flow_business layer(mlflow.py)
import mlflow
import mlflow.sklearn
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
import sqlite3
import pandas as pd

# Connect to the SQLite database
conn = sqlite3.connect("penguin.db")

# Read data from a table using Pandas
data_df = pd.read_sql("SELECT * FROM penguin", conn)

def train_model():
    mlflow.set_experiment("penguin_Classification")
    X = data_df.drop('species_short', axis=1)
    y = data_df['species_short']

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    clf = RandomForestClassifier()

    with mlflow.start_run():
        clf.fit(X_train, y_train)

        # Log model parameters
        mlflow.log_param("n_estimators", clf.n_estimators)
        mlflow.log_param("criterion", clf.criterion)

        # Log model performance metrics
        train_score = clf.score(X_train, y_train)
        test_score = clf.score(X_test, y_test)
        mlflow.log_metric("train_score", train_score)
        mlflow.log_metric("test_score", test_score)

        # Save the model as an artifact
        mlflow.sklearn.log_model(clf, "model")

    return clf, test_score

if __name__ == '__main__':
    clf, accuracy = train_model()
    print(f"Model trained with accuracy: {accuracy}")
    mlflow.log_metric("accuracy", accuracy)
    mlflow.sklearn.log_model(clf, "model")
    mlflow.sklearn.log_model(clf, "model", registered_model_name="penguin_model")
    mlflow.sklearn.save_model(clf, "penguin_model_3")

    # Launch MLflow UI
    import os
    os.system("mlflow ui")

"""# presentation layer (we have two options)

### 1. With Streamlit
"""

pip install streamlit

import streamlit as st
import pickle

with open("model.pkl", "rb") as f:
    model = pickle.load(f)

# streamlit
st.title("Penguin Classification")
st.sidebar.title("Input Parameters")

culmen_length_mm = st.sidebar.slider("culmen_length", 0.0, 60.0, 30.0)
culmen_depth_mm = st.sidebar.slider("culmen_depth", 0.0, 25.0, 12.5)
flipper_length_mm = st.sidebar.slider("flipper_length", 0.0, 240.0, 120.0)
body_mass_g = st.sidebar.slider("body_mass", 0.0, 6000.0, 3000.0)

data = [[culmen_length_mm, culmen_depth_mm, flipper_length_mm, body_mass_g]]
prediction = model.predict(data)[0]

if st.sidebar.button("Predict"):
    prediction = model.predict(data)
    st.write({"prediction": prediction})

"""### 2. With HTML and CSS"""

# index.html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Penguin Classification</title>
    <link rel="stylesheet" href="static/style.css">
</head>
<body>
    <h1>Penguin Classification</h1>
    <form action="/classify" method="post">
        <label for="culmen_length_mm">culmen_length:</label>
        <input type="number" step="0.1" id="culmen_length_mm" name="culmen_length_mm" required><br><br>
        <label for="culmen_depth_mm">culmen_depth:</label>
        <input type="number" step="0.1" id="culmen_depth_mm" name="culmen_depth_mm" required><br><br>
        <label for="flipper_length_mm">flipper_length:</label>
        <input type="number" step="0.1" id="flipper_length_mm" name="flipper_length_mm" required><br><br>
        <label for="body_mass_g">body_mass:</label>
        <input type="number" step="0.1" id="body_mass_g" name="body_mass_g" required><br><br>
        <input type="submit" value="Classify">
    </form>
    {% if prediction %}
    <h2>Prediction: {{ prediction }}</h2>
    {% endif %}
</body>
</html>

# style.css

body {
    font-family: Arial, sans-serif;
    max-width: 600px;
    margin: 0 auto;
    padding: 20px;
}

input[type=number], input[type=submit] {
    width: 100%;
    padding: 5px;
    margin: 5px 0;
    box-sizing: border-box;
}

input[type=submit] {
    background-color: #4CAF50;
    color: white;
    cursor: pointer;
}

from flask import Flask, render_template, request, jsonify
import pickle
import sqlite3

app = Flask(__name__)

with open("model.pkl", "rb") as f:
    model = pickle.load(f)

@app.route("/", methods=["GET"])
def index():
    return render_template("index.html", prediction=None)

@app.route("/classify", methods=["POST"])
def classify():
    culmen_length_mm = float(request.form["culmen_length_mm"])
    culmen_depth_mm = float(request.form["culmen_depth_mm"])
    flipper_length_mm = float(request.form["flipper_length_mm"])
    body_mass_g = float(request.form["body_mass_g"])

    data = [[culmen_length_mm, culmen_depth_mm, flipper_length_mm, body_mass_g]]
    prediction = model.predict(data)[0]

    # Save the data to the database
    connection = sqlite3.connect("penguin.db")
    cursor = connection.cursor()
    cursor.execute("INSERT INTO penguin (culmen_length_mm, culmen_depth_mm, flipper_length_mm, body_mass_g, species_short) VALUES (?, ?, ?, ?, ?)",
                   (culmen_length_mm, culmen_depth_mm, flipper_length_mm, body_mass_g, prediction))
    connection.commit()
    connection.close()

    return jsonify({"prediction": prediction})


if __name__ == "__main__":
    app.run(debug=True, port=5006)